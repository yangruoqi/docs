{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动静结合\n",
    "\n",
    "[![下载Notebook](https://gitee.com/mindspore/docs/raw/tutorials-develop/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/tutorials-develop/tutorials/zh_cn/mindspore_combine.ipynb)&emsp;\n",
    "[![下载样例代码](https://gitee.com/mindspore/docs/raw/tutorials-develop/resource/_static/logo_download_code.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/tutorials-develop/tutorials/zh_cn/mindspore_combine.py)&emsp;\n",
    "[![查看源文件](https://gitee.com/mindspore/docs/raw/tutorials-develop/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/tutorials-develop/tutorials/source_zh_cn/advance/pynative_graph/combine.ipynb)\n",
    "\n",
    "当前在业界支持动态图和静态图两种模式，动态图通过解释执行，具有动态语法亲和性，表达灵活；静态图使用jit(just in time)编译优化执行，偏静态语法，在语法上有较多限制。动态图和静态图的编译流程不一致，导致语法约束也不一致。\n",
    "\n",
    "MindSpore针对动态图和静态图模式，首先统一API表达，在两种模式下使用相同的API；其次统一动态图和静态图的底层微分机制。\n",
    "\n",
    "![dynamic](https://gitee.com/mindspore/docs/raw/tutorials-develop/tutorials/source_zh_cn/advance/pynative_graph/images/framework1.png)\n",
    "\n",
    "## 实现原理\n",
    "\n",
    "MindSpore支持使用`ms_function`装饰器来修饰需要用静态图执行的对象，从而实现动静结合的目的。下面我们通过一个简单的动静结合的示例来介绍其实现原理。示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init x:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "x:\n",
      " [[8. 8. 8.]\n",
      " [8. 8. 8.]\n",
      " [8. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context, Tensor, ms_function\n",
    "\n",
    "class Add(nn.Cell):\n",
    "    \"\"\"自定义类实现x自身相加\"\"\"\n",
    "    def construct(self, x):\n",
    "        x = x + x\n",
    "        return x\n",
    "\n",
    "class Mul(nn.Cell):\n",
    "    \"\"\"自定义类实现x自身相乘\"\"\"\n",
    "    @ms_function  # 使用ms_function修饰，此函数以静态图方式执行\n",
    "    def construct(self, x):\n",
    "        x = x * x\n",
    "        return x\n",
    "\n",
    "class Test(nn.Cell):\n",
    "    \"\"\"自定义类实现x先Add(x)，后Mul(x)，再Add(x)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__()\n",
    "        self.add = Add()\n",
    "        self.mul = Mul()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.add(x)\n",
    "        x = self.mul(x)\n",
    "        x = self.add(x)\n",
    "        return x\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE)\n",
    "x = Tensor(np.ones([3, 3], dtype=np.float32))\n",
    "print(\"init x:\\n\", x)\n",
    "net = Test()\n",
    "x = net(x)\n",
    "print(\"\\nx:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的打印结果可以看出，经过Test运算后，x最终值为每个元素都是8的3\\*3矩阵。该用例按照执行序，编译的方式如下图所示：\n",
    "\n",
    "![msfunction](https://gitee.com/mindspore/docs/raw/tutorials-develop/tutorials/source_zh_cn/advance/pynative_graph/images/ms_function.png)\n",
    "\n",
    "被`ms_function`修饰的函数将会按照静态图的方式进行编译和执行。如果网络涉及到反向求导，被`ms_function`修饰的部分也将以整图的形式来生成反向图，并与前后单个算子的反向图连成一个整体后被下发执行。其中，缓存的策略与静态图的缓存策略一致，相同的函数对象在输入Shape和Type信息一致时，编译的图结构将会被缓存。\n",
    "\n",
    "## 相互转换\n",
    "\n",
    "在MindSpore中，我们可以通过控制模式输入参数`context.set_context`来切换执行使用动态图还是静态图。\n",
    "\n",
    "由于在静态图下，对于Python语法有所限制，因此从动态图切换成静态图时，需要符合静态图的语法限制，才能正确使用静态图来进行执行。\n",
    "\n",
    "> 更多静态图的语法限制可以参考[静态图语法支持](https://www.mindspore.cn/docs/note/zh-CN/master/static_graph_syntax_support.html)。\n",
    "\n",
    "### 使用`ms_function`装饰器\n",
    "\n",
    "MindSpore支持在动态图下使用静态编译的方式来进行混合执行，通过使用`ms_function`装饰符来修饰需要用静态图来执行的函数对象，即可实现动态图和静态图的混合执行。\n",
    "\n",
    "#### 1. 修饰独立函数\n",
    "\n",
    "使用`ms_function`装饰器时，可以对独立定义的函数进行修饰，使其在Graph模式下运行，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 7. 9.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context, Tensor, ms_function\n",
    "\n",
    "# 设置运行模式为动态图模式\n",
    "context.set_context(mode=context.PYNATIVE_MODE)\n",
    "\n",
    "# 使用装饰器，指定静态图模式下执行\n",
    "@ms_function\n",
    "def add_func(x, y):\n",
    "    return ops.add(x, y)\n",
    "\n",
    "x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))\n",
    "y = Tensor(np.array([4.0, 5.0, 6.0]).astype(np.float32))\n",
    "\n",
    "out = add_func(x, y)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的示例代码中，虽然一开始设置了运行模式为动态图模式，但是由于使用了`ms_function`装饰器对函数`add_func(x, y)`进行了修饰，所以函数`add_func(x, y)`仍然是以静态图模式运行。\n",
    "\n",
    "#### 2. 修饰Cell的成员函数\n",
    "\n",
    "使用`ms_function`装饰器时，可以对`Cell`的成员函数进行修饰，示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer result:\n",
      " [5. 7. 9.]\n",
      "Gradient result:\n",
      "Grad x Tensor1:\n",
      " [1. 1. 1.]\n",
      "Grad y Tensor2:\n",
      " [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context, Tensor, ms_function\n",
    "\n",
    "# 设置运行模式为动态图模式\n",
    "context.set_context(mode=context.PYNATIVE_MODE)\n",
    "\n",
    "class Add(nn.Cell):\n",
    "\n",
    "    @ms_function # 使用装饰器，指定静态图模式下执行\n",
    "    def construct(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))\n",
    "y = Tensor(np.array([4.0, 5.0, 6.0]).astype(np.float32))\n",
    "\n",
    "grad_ops = ops.GradOperation(get_all=True)  # 定义求导操作\n",
    "net = Add()\n",
    "grad_out = grad_ops(net)(x, y)\n",
    "\n",
    "print(\"Infer result:\\n\", net(x, y))\n",
    "\n",
    "print(\"Gradient result:\")\n",
    "print(\"Grad x Tensor1:\\n\", grad_out[0])  # 对x求导\n",
    "print(\"Grad y Tensor2:\\n\", grad_out[1])  # 对y求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的打印结果可以看出，x与y相加的结果为\\[5, 7, 9\\]， 对x求导的结果和对y求导的结果相同，都为\\[1, 1, 1\\]。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}