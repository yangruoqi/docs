{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义网络模型\n",
    "\n",
    "[![下载Notebook](https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/tutorials-develop/tutorials/zh_cn/mindspore_customized_model.ipynb)&emsp;[![下载样例代码](https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_download_code.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/tutorials-develop/tutorials/zh_cn/mindspore_customized_model.py)&emsp;[![查看源文件](https://gitee.com/mindspore/docs/raw/master/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/tutorials-develop/tutorials/source_zh_cn/intermediate/build_net/customized_model.ipynb)\n",
    "\n",
    "构建网络时，通过继承`nn.Cell`类，在`__init__`构造函数中申明各个层的定义，在`construct`中实现层之间的连接关系，完成神经网络的前向构造。且初始化网络之前需要配置`context`参数，用于控制程序执行的策略，如配置静态图或动态图模式，配置网络运行的硬件环境等。本章主要介绍配置信息和构建网络的几种方法。\n",
    "\n",
    "## 配置信息\n",
    "\n",
    "初始化网络之前要配置`context`参数，用于控制程序执行的策略，本节主要介绍执行模式管理和硬件管理。\n",
    "\n",
    "### 执行模式管理\n",
    "\n",
    "MindSpore支持Graph和PyNative两种运行模式。Graph模式是MindSpore的主模式，而PyNative模式用于调试等用途。\n",
    "\n",
    "- Graph模式：静态图模式或者图模式，将神经网络模型编译成一整张图，然后下发执行。该模式利用图优化等技术提高运行性能，同时有助于规模部署和跨平台运行。默认为Graph模式。\n",
    "\n",
    "- PyNative模式：动态图模式，将神经网络中的各个算子逐一下发执行，方便用户编写和调试神经网络模型。\n",
    "\n",
    "#### 模式选择\n",
    "\n",
    "通过配置context参数可以控制程序运行的模式。Graph和PyNative两种模式的区别主要有：\n",
    "\n",
    "- 使用场景：Graph模式需要一开始就构建好网络结构，然后框架做整图优化和执行，比较适合网络固定没有变化，且需要高性能的场景。而PyNative模式逐行执行算子，支持执行单算子、普通函数和网络，以及单独求梯度的操作。\n",
    "\n",
    "- 网络执行：Graph模式和PyNative模式在执行相同的网络和算子时，精度效果是一致的。由于Graph模式运用了图优化、计算图整图下沉等技术，Graph模式执行网络的性能和效率更高。\n",
    "\n",
    "- 代码调试：在脚本开发和网络流程调试中，推荐使用PyNative模式进行调试。在PyNative模式下，可以方便地设置断点，获取网络执行的中间结果，也可以通过pdb的方式对网络进行调试。而Graph模式无法设置断点，只能先指定算子进行打印，然后在网络执行完成后查看输出结果。\n",
    "\n",
    "使用Graph模式时，将context中的运行模式设置为`GRAPH_MODE`，需要使用`nn.Cell`类，并在`construct`函数中编写执行代码， 或者调用`@ms_function`装饰器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. 10. 18.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context, Tensor\n",
    "\n",
    "# 设置运行模式为静态图模式，硬件设备为CPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.mul = ops.Mul()\n",
    "\n",
    "    def construct(self, x, y):\n",
    "        return self.mul(x, y)\n",
    "\n",
    "x = Tensor(np.array([1.0, 2.0, 3.0]).astype(np.float32))\n",
    "y = Tensor(np.array([4.0, 5.0, 6.0]).astype(np.float32))\n",
    "\n",
    "net = Net()\n",
    "print(net(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模式切换\n",
    "\n",
    "MindSpore提供了静态图和动态图统一的编码方式，大大增加了静态图和动态图的可兼容性，用户无需开发多套代码，仅变更一行代码便可切换静态图/动态图模式。模式切换时，请留意目标模式的[约束](https://www.mindspore.cn/docs/note/zh-CN/master/static_graph_syntax_support.html)。例如，PyNative模式不支持数据下沉等。\n",
    "\n",
    "MindSpore处于静态图模式时，可以通过 `context.set_context(mode=context.PYNATIVE_MODE)`切换为动态图模式；同样地，MindSpore处于动态图模式时，可以通过`context.set_context(mode=context.GRAPH_MODE)`切换为静态图模式。\n",
    "\n",
    "### 硬件管理\n",
    "\n",
    "硬件管理部分主要包括`device_target`和`device_id`两个参数。\n",
    "\n",
    "- `device_target`： 待运行的目标设备，支持`Ascend`、`GPU`和`CPU`，可以根据实际环境情况设置。\n",
    "\n",
    "- `device_id`： 表示目标设备ID，其值在[0, `device_num_per_host` - 1]范围内，`device_num_per_host`表示服务器的总设备数量，`device_num_per_host`的值不能超过4096，`device_id`默认为0。在非分布式模式执行的情况下，为了避免设备的使用冲突，可以通过设置`device_id`决定程序执行的设备ID。\n",
    "\n",
    "代码样例如下：\n",
    "\n",
    "```Python\n",
    "from mindspore import context\n",
    "context.set_context(device_target=\"Ascend\", device_id=6)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络\n",
    "\n",
    "MindSpore的`nn.Cell`类是构建所有网络的基类，也是网络的基本单元，构建网络时，可以继承`nn.Cell`类，在`__init__`构造函数中申明各个层的定义，在`construct`中实现层之间的连接关系，完成神经网络的前向构造。`mindspore.ops`模块提供了基础算子的实现，如神经网络算子、数组算子和数学算子等。`mindspore.nn`模块实现了对基础算子的进一步封装，用户可以根据需要，灵活使用不同的算子。同时，为了更好地构建和管理复杂的网络，`mindspore.nn`提供了两种容器对网络中的子模块或模型层进行管理，分别为`nn.CellList`和`nn.SequentialCell`两种方式。\n",
    "\n",
    "### 使用ops算子构建网络\n",
    "\n",
    "[mindspore.ops](https://www.mindspore.cn/docs/api/zh-CN/master/api_python/mindspore.ops.html)模块提供了基础算子的实现，如神经网络算子、数组算子和数学算子等。用户可使用`mindspore.ops`中的算子来构建一个简单的网络$f(x)=x^2+w$，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  6. 11.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import context, Parameter, ops, Tensor, nn\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.mul = ops.Mul()\n",
    "        self.add = ops.Add()\n",
    "        self.weight = Parameter(Tensor(np.array([2, 2, 2]), mindspore.float32))\n",
    "\n",
    "    def construct(self, x):\n",
    "        return self.add(self.mul(x, x), self.weight)\n",
    "\n",
    "net = Net()\n",
    "input = Tensor(np.array([1, 2, 3]))\n",
    "output = net(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用nn层构建网络\n",
    "\n",
    "尽管`mindspore.ops`模块提供的多样算子可以基本满足网络构建的诉求，但为了在复杂的深度网络中提供更方便易用的接口，[mindspore.nn](https://www.mindspore.cn/docs/api/zh-CN/master/api_python/mindspore.nn.html#)对`mindspore.ops`算子进行了进一步的封装。`mindspore.nn`模块包括了卷基层（如`nn.Conv2d`）、池化层（`nn.MaxPool2d`）、非线性激活函数（如`nn.ReLU`）、损失函数（如`nn.LossBase`）、优化器（如`nn.Momentum`）等，为用户的使用提供了便利。使用`mindspore.nn`模块构建一个Conv+BN(Batch Normalization)+ReLu模型网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBNReLU<\n",
      "  (conv): Conv2d<input_channels=3, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "  (bn): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=bn.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "  (relu): ReLU<>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import Tensor, nn\n",
    "\n",
    "class ConvBNReLU(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        out = self.relu(x)\n",
    "        return out\n",
    "\n",
    "net = ConvBNReLU()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用容器构建网络\n",
    "\n",
    "为了便于管理和组成更复杂的网络，`mindspore.nn`提供了容器对网络中的子模型块或模型层进行管理，有`nn.CellList`和`nn.SequentialCell`两种方式。\n",
    "\n",
    "#### `nn.CellList`构建网络\n",
    "\n",
    "使用`nn.CellList`构造的Cell既可以是模型层，也可以是构建的网络子块。`nn.CellList`支持`append`方法（在列表末尾添加一个Cell），`extend`方法（在列表末尾添加多个Cell）和`insert`方法（在列表给定的索引之前插入给定的Cell）。在执行网络时，可以在construct方法里，使用for循环，运行输出结果。\n",
    "\n",
    "- `append(cell)`：在列表末尾添加一个cell。\n",
    "- `extend（cells)`：将cells添加至列表末尾。\n",
    "- `insert(index, cell)`：在列表给定的索引之前插入给定的cell。\n",
    "\n",
    "如下使用`nn.CellList`构建并执行一个网络，依次包含一个之前定义的模型子块ConvBNReLU、一个Conv2d层、一个BatchNorm2d层和一个ReLU层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet<\n",
      "  (build_block): CellList<\n",
      "    (0): ConvBNReLU<\n",
      "      (conv): Conv2d<input_channels=3, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "      (bn): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=build_block.0.bn.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=build_block.0.bn.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=build_block.0.bn.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=build_block.0.bn.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "      (relu): ReLU<>\n",
      "      >\n",
      "    (1): Conv2d<input_channels=64, output_channels=4, kernel_size=(4, 4), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "    (2): BatchNorm2d<num_features=4, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=build_block.2.gamma, shape=(4,), dtype=Float32, requires_grad=True), beta=Parameter (name=build_block.2.beta, shape=(4,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=build_block.2.moving_mean, shape=(4,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=build_block.2.moving_variance, shape=(4,), dtype=Float32, requires_grad=False)>\n",
      "    (3): ReLU<>\n",
      "    >\n",
      "  >\n",
      "(1, 4, 64, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import Tensor, nn\n",
    "\n",
    "class MyNet(nn.Cell):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        # 将上一步中定义的ConvBNReLU加入一个列表\n",
    "        layers = [ConvBNReLU()]\n",
    "        # 使用CellList对网络进行管理\n",
    "        self.build_block = nn.CellList(layers)\n",
    "        # 使用append方法添加Conv2d层和ReLU层\n",
    "        self.build_block.append(nn.Conv2d(64, 4, 4))\n",
    "        self.build_block.append(nn.ReLU())\n",
    "        # 使用insert方法在Conv2d层和ReLU层中间插入BatchNorm2d\n",
    "        self.build_block.insert(-1, nn.BatchNorm2d(4))\n",
    "\n",
    "    def construct(self, x):\n",
    "        # for循环执行网络\n",
    "        for layer in self.build_block:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "net = MyNet()\n",
    "print(net)\n",
    "\n",
    "input = Tensor(np.ones([1, 3, 64, 32]), mindspore.float32)\n",
    "output = net(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `nn.SequentialCell`构建网络\n",
    "\n",
    "使用`nn.SequentialCell`构造Cell顺序容器，支持子模块以List或OrderedDict格式作为输入。不同于`nn.CellList`的是，`nn.SequentialCell`类内部实现了construct方法，可以直接输出结果。\n",
    "\n",
    "如下示例使用`nn.SequentialCell`构建一个网络，输入为List，网络结构依次包含一个之前定义的模型子块ConvBNReLU、一个Conv2d层、一个BatchNorm2d层和一个ReLU层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet<\n",
      "  (build_block): SequentialCell<\n",
      "    (0): ConvBNReLU<\n",
      "      (conv): Conv2d<input_channels=3, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "      (bn): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=build_block.0.bn.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=build_block.0.bn.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=build_block.0.bn.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=build_block.0.bn.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "      (relu): ReLU<>\n",
      "      >\n",
      "    (1): Conv2d<input_channels=64, output_channels=4, kernel_size=(4, 4), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "    (2): BatchNorm2d<num_features=4, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=build_block.2.gamma, shape=(4,), dtype=Float32, requires_grad=True), beta=Parameter (name=build_block.2.beta, shape=(4,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=build_block.2.moving_mean, shape=(4,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=build_block.2.moving_variance, shape=(4,), dtype=Float32, requires_grad=False)>\n",
      "    (3): ReLU<>\n",
      "    >\n",
      "  >\n",
      "(1, 4, 64, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import Tensor, nn\n",
    "\n",
    "class MyNet(nn.Cell):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        # 将上一步中定义的ConvBNReLU加入一个列表\n",
    "        layers = [ConvBNReLU()]\n",
    "       # 在列表中添加模型层\n",
    "        layers.extend([\n",
    "            nn.Conv2d(64, 4, 4),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        # 使用SequentialCell对网络进行管理\n",
    "        self.build_block = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        return self.build_block(x)\n",
    "\n",
    "net = MyNet()\n",
    "print(net)\n",
    "\n",
    "input = Tensor(np.ones([1, 3, 64, 32]), mindspore.float32)\n",
    "output = net(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下示例使用`nn.SequentialCell`构建一个网络，输入为OrderedDict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet<\n",
      "  (build_block): SequentialCell<\n",
      "    (ConvBNReLU): ConvBNReLU<\n",
      "      (conv): Conv2d<input_channels=3, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "      (bn): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=build_block.ConvBNReLU.bn.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=build_block.ConvBNReLU.bn.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=build_block.ConvBNReLU.bn.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=build_block.ConvBNReLU.bn.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
      "      (relu): ReLU<>\n",
      "      >\n",
      "    (conv): Conv2d<input_channels=64, output_channels=4, kernel_size=(4, 4), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW>\n",
      "    (norm): BatchNorm2d<num_features=4, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=build_block.norm.gamma, shape=(4,), dtype=Float32, requires_grad=True), beta=Parameter (name=build_block.norm.beta, shape=(4,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=build_block.norm.moving_mean, shape=(4,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=build_block.norm.moving_variance, shape=(4,), dtype=Float32, requires_grad=False)>\n",
      "    (relu): ReLU<>\n",
      "    >\n",
      "  >\n",
      "(1, 4, 64, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import Tensor, nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class MyNet(nn.Cell):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        layers = OrderedDict()\n",
    "        # 将cells加入字典\n",
    "        layers[\"ConvBNReLU\"] = ConvBNReLU()\n",
    "        layers[\"conv\"] = nn.Conv2d(64, 4, 4)\n",
    "        layers[\"norm\"] = nn.BatchNorm2d(4)\n",
    "        layers[\"relu\"] = nn.ReLU()\n",
    "        # 使用SequentialCell对网络进行管理\n",
    "        self.build_block = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        return self.build_block(x)\n",
    "\n",
    "net = MyNet()\n",
    "print(net)\n",
    "\n",
    "input = Tensor(np.ones([1, 3, 64, 32]), mindspore.float32)\n",
    "output = net(input)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
